# The Hard Part: vLLM Parameters

```bash
vllm serve \
  --model Qwen/Qwen3-Coder-30B-A3B-Instruct \
  --gpu-memory-utilization 0.95 \
  --max-model-len 262144 \
  --max-num-seqs 32 \
  --max-num-batched-tokens 65536 \
  --enable-chunked-prefill \
  --disable-log-requests \
  --enable-auto-tool-choice \
  --tool-call-parser qwen3_coder \
  --tensor-parallel-size 1
```

- Finding the right balance between throughput and memory
- Tuning for long context windows (262k tokens)
- Optimizing batch processing parameters
