# Key Repositories (ScanNet POC Focus)

**llm-server**
Local vLLM server for high-performance model inference. GPU-accelerated serving with support for various quantized models.

**llm-server-tui**
Terminal user interface for monitoring and managing the LLM server.

**llm-loadtest**
Inference benchmark suite for testing model performance under various load conditions.

---

**Documentation & Learning:**
https://github.com/martinmose/zeldoc-documentation
- Extensive documentation of the learning process
- Lots to learn, lots of experimentation
- Breaking things to learn faster

**Other repositories:**
- llm-gateway: Smart routing between local and cloud LLM providers
- hetzner-iac: Infrastructure as Code for Hetzner Cloud deployment
